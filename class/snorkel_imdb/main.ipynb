{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snorkel on IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install snorkel datasets scikit-learn pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore the IMDb Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load 2000 training and 500 test examples for speed​\n",
    "\n",
    "imdb = load_dataset(\"imdb\")\n",
    "train = pd.DataFrame(imdb[\"train\"].select(range(2000)))\n",
    "test = pd.DataFrame(imdb[\"test\"].select(range(500)))\n",
    "print(\"Train size:\", len(train), \"Test size:\", len(test))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"<br\\s*/?>\", \" \", text)\n",
    "    text = re.sub(r\"[^\\w\\s']\", \"\", text)\n",
    "    return text.lower()\n",
    "\n",
    "train[\"text\"] = train[\"text\"].apply(clean_text)\n",
    "test[\"text\"] = test[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Labeling Functions (LFs)\n",
    "\n",
    "Create simple heuristics:\n",
    "\n",
    "+ **LF_positive**: labels text as positive if it contains strong positive words.​​\n",
    "+ **LF_negative**: labels text as negative if it contains strong negative words\n",
    "+ **LF_exclaim**: positive if contains “!” more than 2 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function, LFAnalysis\n",
    "from snorkel.labeling import LabelModel\n",
    "\n",
    "ABSTAIN, NEG, POS = -1, 0, 1\n",
    "positive_words = {\"great\",\"excellent\",\"amazing\",\"wonderful\",\"best\",\"fantastic\"}\n",
    "negative_words = {\"bad\",\"terrible\",\"awful\",\"worst\",\"boring\",\"poor\"}\n",
    "\n",
    "@labeling_function()\n",
    "def lf_positive(x):\n",
    "    return POS if any(w in x.text.split() for w in positive_words) else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_negative(x):\n",
    "    return NEG if any(w in x.text.split() for w in negative_words) else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_exclaim(x):\n",
    "    return POS if x.text.count(\"!\") > 2 else ABSTAIN\n",
    "lfs = [lf_positive, lf_negative, lf_exclaim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze LF Coverage & Conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import PandasLFApplier\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_train = applier.apply(train)\n",
    "LFAnalysis(L_train, lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the LabelModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train=L_train, n_epochs=500, log_freq=100, seed=42)\n",
    "\n",
    "# Get probabilistic labels​\n",
    "train_probs = label_model.predict_proba(L_train)\n",
    "train_preds = label_model.predict(L_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train an End-to-End Classifier\n",
    "\n",
    "Use a simple logistic regression on TF-IDF features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Vectorize​\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5_000)\n",
    "X_train = vectorizer.fit_transform(train[\"text\"])\n",
    "y_train = train_preds\n",
    "\n",
    "# Fit classifier​\n",
    "\n",
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test set​\n",
    "\n",
    "X_test = vectorizer.transform(test[\"text\"])\n",
    "y_test = test[\"label\"]\n",
    "preds = clf.predict(X_test)\n",
    "print(classification_report(y_test, preds, target_names=[\"neg\",\"pos\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Weak Supervision vs. Fully Supervised\n",
    "\n",
    "For comparision, train the same classifier on 2,000 true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_fs = LogisticRegression(max_iter=200)\n",
    "clf_fs.fit(X_train, train[\"label\"])\n",
    "fs_preds = clf_fs.predict(X_test)\n",
    "print(\"Fully supervised performance:\")\n",
    "print(classification_report(y_test, fs_preds, target_names=[\"neg\",\"pos\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Visualize confusion matrices and coverage-accuracy trade-off curves"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
